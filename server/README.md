
# Lab 2: Server Concurrency

In this lab you work with a package for the internals of a server, called *goose*.   (Go object server environment?)    The server is single-threaded in the seed code.   Your mission is to use threads in a sensible way to improve server performance.

## Inside the Goose

Real servers are built from generic server frameworks with pluggable *server applications*.   The framework and configured application(s) run together as a process.   The framework passes incoming requests to the configured application (e.g., based on a prefix of a requested URL).

Goose ships with one trivial application: each request pauses (blocks) a server thread for a configured amount of time (the *wait demand*), and/or optionally burns on a CPU core for a configured amount of time (the *work demand*).   The idea is to mimic the performance behavior of a real application, without bothering to do anything interesting.

The goose server body runs as a goroutine and receives requests over a channel.   In a real server those requests arrive on the network and pass through the network stack before delivery to the server application.   In goose they are generated by a *synthetic load generator*, also a goroutine.  The load generator (*loadgen*) also gathers statistics of server response time and throughput, and prints them along with a histogram of response times.

Loadgen is configured to generate requests with specified statistical properties.   These properties match assumptions of queuing theory, enabling us to compare observed performance to predictions of analytical models.   Specifically, the per-request demands and inter-arrival times are exponentially distributed around specified means.

There is no need to study the loadgen or stats code.

## The Server

The server body is in the source file `goose.go`.    It is the only source file you need to concern yourself with.

`ReqHandler` runs as a goroutine.   It uses `range` to receive requests from loadgen over a request channel until loadgen closes the channel.

The server handles each request by passing it to func `serve`, which examines the request and does the following:
- Expend the demanded resources.
- Send a reply to a client channel specified in the request.   Here the reply is simply a copy of the request.
- Return to the caller.

Thus the server is *single-threaded*: it can handle at most one request at a time.

## How Many Threads?

A server can use threads to work on multiple requests at once, in parallel.   That enables the server to take advantage of multiple cores in modern server machines.  Or, if the wait demand represents blocking for a storage device (e.g., to read a requested web page from disk), threads enable the server to use multiple devices in parallel.

Servers serve many clients, whose requests arrive independently and can (mostly) be handled independently.   In this sense server workloads are ``embarrasingly parallel'' and any respectable server takes advantage of that.

A natural way to take advantage of parallelism is to create a thread (goroutine) for each incoming request, and call `serve` in that goroutine.   However, that has the drawback that under heavy load a vast number of goroutines contend for cores.   Why is that a problem?    It can lead to a nasty phenomenon called *thrashing*.  The system may switch a thread off its core in order to run another ready thread.   But once a server starts working on a request, it is better to let it finish what it started and get the reply out to the client rather than disrupt it by starting work on a new request.  

Therefore, it is desirable to limit the number of active goroutines and active requests in service to some parameter (*maxConcurrent*).  In practice, the parameter *maxConcurrent* is chosen to match the available parallelism within the server, e.g., not less than the number of CPU cores, but not too many.   Like notes in a musical piece, the number of active threads should be neither more nor less than required.   (cf. Mozart in Amadeus)

One way to limit parallelism is to create *maxConcurrent* threads at the start and then dispatch requests in sequence to those threads as they become available.    In a server context this structure is called *workcrew*.   Go uses an analogous approach internally to schedule goroutines onto operating system threads.

But what if all the threads are busy working on requests?   Then the code needs a queue of pending requests to dispatch.   That's more code to write.  Go makes it easy to write queues that generate a lot of garbage exhaust, but it is a little more involved to queue requests efficiently.

A simpler approach, which we recommend for this lab, is to fire a goroutine for each request, but use synchronization to block them as needed to ensure that at most *maxConcurrent* of them are in `serve` at any given time, and the others block until a slot opens up.    For this purpose we recommend a counting semaphore.   You could use the version in go-samples, or use a channel as a counting semaphore.  That way, all the queuing is handled by the Go primitives behind the curtain.

You may find the following link to be valuable and of interest: [On concurrency in Go HTTP servers](https://eli.thegreenplace.net/2019/on-concurrency-in-go-http-servers/).

## Performance

You can run your server:

```
cd server
go serveload.go
```

Each run generates a load of 1000 requests matching the configured rate in requests per second (*offered load* or *lambda*).

Try some different parameters.   Does the performance match the analytical predictions discussed in class?
- For a given offered load lambda, what happens as you add more threads (increase maxConcurrent)?
- What is the maximum throughput your server can deliver (called *peak rate* or *saturating load*) for a given demand mean (say, 10ms)?
- How does varying maxConcurrent affect the peak rate?   Is the improvement from concurrency limited by the number of cores in this case?   Why or why not?
- At lambda below the peak rate, the parameter *rho* represents the *load factor*, or ratio of lambda to the peak rate.   Rho is a number between 0 and 1, roughly corresponding to utilization: what percentage of the server's capacity is being used?
- What is the relationship between load factor and mean response time?   
- What is the effect of maxConcurrent on that relationship?   
- What is the relationship between load factor and the tightness of the response time distribution?

The analytical models that predict the mean response time for a single-threaded server (maxConcurrent == 1) is called M/M/1: it predicts that the mean response time is the mean demand divided by (1-rho).  For concurrent servers the model is called M/M/c and it predicts lower mean response times and a more complex relationship.


## Testing

To run tests with your code, use the following go command to execute the goose server program.
```
go run serveload.go <iatMean> <demandMean> <maxConcurrent>
```

- iatMean: mean inter-arrival time in milliseconds. The time interval between each arrival of the request. lambda (offered load) is calculated as 1/iatMean.
- demandMean: mean WaitDemand in milliseconds. The time for wait demand in the request. 
- maxConcurrent: mean the maximum number of concurrent requests in the server. The maximum throughput is calculated as maxConcurrent/demandMean.

For example, try run 
```
go run serveload.go 16 10 4
```

and you will see the following output
```
sent=1000 offered load lambda=60.82/sec, clear time=12ms
sent=1000 skipped=0 throughput=61/sec meanRT=26.598ms
      0-10ms |████████████████████████████████████████████████████████████ 320
     10-20ms |██████████████████████████████████████ 204
     20-30ms |███████████████████████████ 149
     30-40ms |███████████████████ 102
     40-50ms |███████████ 63
     50-60ms |██████████ 54
     60-70ms |██████ 36
     70-80ms |████ 23
     80-90ms |██ 14
    90-100ms |█ 8
      100ms+ |█████ 27
total: 1000
```

Tweak the number for different iatMean, demandMean and maxConcurrent. What do you observe and try to relate your findings with the previous questions.

There is also a python script to run autograder for the lab. Run the following command to use it. 
```
python3 run_tests.py
```

The scripts and test cases are identical to the ones on Gradescope. There are no hidden tests.

## Submission

Summit your code to Coursework(Gitlab) with Git. After that go to gradescope and click submit through gitlab.

```
git add .
git commit -am "<your commit message>"
git push
```
